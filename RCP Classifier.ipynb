{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import nltk\n",
    "import glob\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = glob.glob(\"data/clean_positive_train_*.csv\")\n",
    "neg_files = glob.glob(\"data/clean_negative_train_*.csv\")\n",
    "\n",
    "df_pos_list = [pd.read_csv(open(fp, 'r'), encoding='utf-8', engine='c') for fp in pos_files[:10]]\n",
    "df_neg_list = [pd.read_csv(open(fp, 'r'), encoding='utf-8', engine='c') for fp in neg_files[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make features for each dataset that we have, calculating all these features takes a long long time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>parent_ups</th>\n",
       "      <th>parent_controversiality</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>profanity</th>\n",
       "      <th>profanity_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>2.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.940550</td>\n",
       "      <td>123.437950</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.144450</td>\n",
       "      <td>2.213078e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500013</td>\n",
       "      <td>41.501038</td>\n",
       "      <td>0.031608</td>\n",
       "      <td>0.236471</td>\n",
       "      <td>273.303875</td>\n",
       "      <td>0.045779</td>\n",
       "      <td>0.274485</td>\n",
       "      <td>0.351554</td>\n",
       "      <td>2.556282e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-342.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.092762e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.562152e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.199541e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>129.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.229088e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5295.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score           ups  controversiality  parent_score  \\\n",
       "count  20000.000000  20000.000000      20000.000000  20000.000000   \n",
       "mean       0.500000     32.500000          0.001000      0.940550   \n",
       "std        0.500013     41.501038          0.031608      0.236471   \n",
       "min        0.000000     -9.000000          0.000000      0.000000   \n",
       "25%        0.000000     -9.000000          0.000000      1.000000   \n",
       "50%        0.500000     32.500000          0.000000      1.000000   \n",
       "75%        1.000000     74.000000          0.000000      1.000000   \n",
       "max        1.000000     74.000000          1.000000      1.000000   \n",
       "\n",
       "         parent_ups  parent_controversiality     sentiment     profanity  \\\n",
       "count  20000.000000             20000.000000  20000.000000  20000.000000   \n",
       "mean     123.437950                 0.002100      0.036857      0.144450   \n",
       "std      273.303875                 0.045779      0.274485      0.351554   \n",
       "min     -342.000000                 0.000000     -1.000000      0.000000   \n",
       "25%       10.000000                 0.000000     -0.025000      0.000000   \n",
       "50%       42.000000                 0.000000      0.000000      0.000000   \n",
       "75%      129.250000                 0.000000      0.150000      0.000000   \n",
       "max     5295.000000                 1.000000      1.000000      1.000000   \n",
       "\n",
       "       profanity_prob  \n",
       "count    2.000000e+04  \n",
       "mean     2.213078e-01  \n",
       "std      2.556282e-01  \n",
       "min      1.092762e-20  \n",
       "25%      7.562152e-02  \n",
       "50%      1.199541e-01  \n",
       "75%      2.229088e-01  \n",
       "max      1.000000e+00  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = [pd.concat([df_pos, df_neg]) for (df_pos, df_neg) in zip(df_pos_list, df_neg_list)]\n",
    "print(len(df))\n",
    "\n",
    "# Now we have scrambeled dataframes of 20000 entries, with features such as binary scores and profanity\n",
    "df[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>parent_ups</th>\n",
       "      <th>parent_controversiality</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>profanity</th>\n",
       "      <th>profanity_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>work guys last year confirm brightest lights c...</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>bad enough lpt never even heard link shortener...</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seems kinda useless unless live bathroom brush...</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>random button power button tv remote yes prett...</td>\n",
       "      <td>1</td>\n",
       "      <td>441</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seriously least hitler killed hitler</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>please dont refer things hitler x bad taste ea...</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.477267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dont worry legitimate hurricane</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>florida suffering one two shitstorms time</td>\n",
       "      <td>1</td>\n",
       "      <td>391</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.205006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir ip adress 127001 hes good</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>start 127</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.192113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score  ups  \\\n",
       "0  work guys last year confirm brightest lights c...      1   74   \n",
       "1  seems kinda useless unless live bathroom brush...      1   74   \n",
       "2               seriously least hitler killed hitler      1   74   \n",
       "3                    dont worry legitimate hurricane      1   74   \n",
       "4                      sir ip adress 127001 hes good      1   74   \n",
       "\n",
       "   controversiality                                        parent_text  \\\n",
       "0                 0  bad enough lpt never even heard link shortener...   \n",
       "1                 0  random button power button tv remote yes prett...   \n",
       "2                 0  please dont refer things hitler x bad taste ea...   \n",
       "3                 0          florida suffering one two shitstorms time   \n",
       "4                 0                                          start 127   \n",
       "\n",
       "   parent_score  parent_ups  parent_controversiality  sentiment  profanity  \\\n",
       "0             1         137                        0   0.000000          0   \n",
       "1             1         441                        0  -0.181818          0   \n",
       "2             1          71                        0  -0.250000          0   \n",
       "3             1         391                        0   0.000000          0   \n",
       "4             1          21                        0   0.700000          0   \n",
       "\n",
       "   profanity_prob  \n",
       "0        0.021070  \n",
       "1        0.059085  \n",
       "2        0.477267  \n",
       "3        0.205006  \n",
       "4        0.192113  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, lower=True, split=' ', document_count=0)\n",
    "# Create the word_index list based on all our data\",\n",
    "text_data = [np.array2string(df[single_df]['text'].values.astype(str)) for single_df in range(len(df))]\n",
    "text_data = ' '.join(text_data)\n",
    "tokenizer.fit_on_texts(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "feature_data = [np.array([])]\n",
    "score_data = [np.array([])]\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "# Iterate over each DataFrame\n",
    "for n in range(len(df)):\n",
    "    \n",
    "    cur_df = df[n]\n",
    "    print(n)\n",
    "    df[n].dropna(axis=0, inplace=True)\n",
    "    size = len(cur_df)\n",
    "    # Iterate over each row in the DataFrame\n",
    "    f_data = np.array([])\n",
    "    for index, row in cur_df.iterrows():\n",
    "        sentiment = row['sentiment']\n",
    "        profanity = row['profanity_prob']\n",
    "        features = np.hstack((sentiment, profanity))\n",
    "        \n",
    "        if index == 0:\n",
    "            f_data = features\n",
    "            s_data = row['score']\n",
    "        else:\n",
    "            f_data = np.vstack([f_data, features])\n",
    "            s_data = np.vstack([s_data, row['score']])\n",
    "    if n == 0:\n",
    "        feature_data[0] = f_data\n",
    "        score_data[0] = s_data\n",
    "    else:\n",
    "        feature_data.append(f_data)\n",
    "        score_data.append(s_data)\n",
    "            \n",
    "    X_train_tmp, X_test_tmp, y_train_tmp, y_test_tmp = train_test_split(feature_data[n], score_data[n], test_size=0.20, random_state=42)\n",
    "    X_train.append(X_train_tmp)\n",
    "    X_test.append(X_test_tmp)\n",
    "    y_train.append(y_train_tmp)\n",
    "    y_test.append(y_test_tmp)\n",
    "\n",
    "    X_train[n] = tf.keras.utils.normalize(X_train[n], axis=1)\n",
    "    X_test[n] = tf.keras.utils.normalize(X_test[n], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9851, 2)\n",
      "(9851, 1)\n",
      "(9852, 2)\n",
      "(9852, 1)\n",
      "(9827, 2)\n",
      "(9827, 1)\n",
      "(9821, 2)\n",
      "(9821, 1)\n"
     ]
    }
   ],
   "source": [
    "for n in range(0,4):\n",
    "    print(feature_data[n].shape)\n",
    "    print(score_data[n].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally we can do some neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(24, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7880 samples, validate on 1970 samples\n",
      "Epoch 1/40\n",
      "7880/7880 [==============================] - 0s 5us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "7880/7880 [==============================] - 0s 6us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "7880/7880 [==============================] - 0s 4us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "7880/7880 [==============================] - 0s 5us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "7880/7880 [==============================] - 0s 4us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0002e-07 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0006e-07 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "7880/7880 [==============================] - 0s 4us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0002e-07 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0002e-07 - acc: 1.0000 - val_loss: 1.0002e-07 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "7880/7880 [==============================] - 0s 4us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0002e-07 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0002e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "7880/7880 [==============================] - 0s 4us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0003e-07 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "7880/7880 [==============================] - 0s 4us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "7880/7880 [==============================] - 0s 4us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "7880/7880 [==============================] - 0s 4us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0006e-07 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "7880/7880 [==============================] - 0s 4us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0000e-07 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0001e-07 - acc: 1.0000 - val_loss: 1.0002e-07 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "7880/7880 [==============================] - 0s 3us/sample - loss: 1.0000e-07 - acc: 1.0000 - val_loss: 1.0001e-07 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "test_number = 4\n",
    "\n",
    "train_data = X_train[test_number]\n",
    "test_data = X_test[test_number]\n",
    "\n",
    "history = model.fit(train_data,\n",
    "                    y_train[test_number],\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(test_data, y_test[test_number]),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
