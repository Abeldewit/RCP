{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.read_csv(open('clean_positive_train.csv','r'), encoding='utf-8', engine='c')\n",
    "df_neg = pd.read_csv(open('clean_negative_train.csv','r'), encoding='utf-8', engine='c')\n",
    "\n",
    "df_pos['text'] = df_pos['text'].astype(str)\n",
    "df_pos['parent_text'] = df_pos['parent_text'].astype(str)\n",
    "\n",
    "df_neg['text'] = df_neg['text'].astype(str)\n",
    "df_neg['parent_text'] = df_neg['parent_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>parent_ups</th>\n",
       "      <th>parent_controversiality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49999.0</td>\n",
       "      <td>49999.000000</td>\n",
       "      <td>49999.000000</td>\n",
       "      <td>49999.000000</td>\n",
       "      <td>49999.000000</td>\n",
       "      <td>49999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>198.448509</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.990240</td>\n",
       "      <td>369.154003</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>256.498200</td>\n",
       "      <td>0.004472</td>\n",
       "      <td>0.098311</td>\n",
       "      <td>530.071252</td>\n",
       "      <td>0.026449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8907.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>419.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4865.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9531.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         score           ups  controversiality  parent_score    parent_ups  \\\n",
       "count  49999.0  49999.000000      49999.000000  49999.000000  49999.000000   \n",
       "mean       1.0    198.448509          0.000020      0.990240    369.154003   \n",
       "std        0.0    256.498200          0.004472      0.098311    530.071252   \n",
       "min        1.0     66.000000          0.000000      0.000000  -8907.000000   \n",
       "25%        1.0     83.000000          0.000000      1.000000     84.000000   \n",
       "50%        1.0    116.000000          0.000000      1.000000    185.000000   \n",
       "75%        1.0    201.000000          0.000000      1.000000    419.000000   \n",
       "max        1.0   4865.000000          1.000000      1.000000   9531.000000   \n",
       "\n",
       "       parent_controversiality  \n",
       "count             49999.000000  \n",
       "mean                  0.000700  \n",
       "std                   0.026449  \n",
       "min                   0.000000  \n",
       "25%                   0.000000  \n",
       "50%                   0.000000  \n",
       "75%                   0.000000  \n",
       "max                   1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>parent_ups</th>\n",
       "      <th>parent_controversiality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49999.0</td>\n",
       "      <td>49999.000000</td>\n",
       "      <td>49999.000000</td>\n",
       "      <td>49999.000000</td>\n",
       "      <td>49999.000000</td>\n",
       "      <td>49999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-14.564351</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.912598</td>\n",
       "      <td>66.805736</td>\n",
       "      <td>0.002740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>15.274812</td>\n",
       "      <td>0.038183</td>\n",
       "      <td>0.282426</td>\n",
       "      <td>216.581912</td>\n",
       "      <td>0.052274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-634.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1622.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14776.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         score           ups  controversiality  parent_score    parent_ups  \\\n",
       "count  49999.0  49999.000000      49999.000000  49999.000000  49999.000000   \n",
       "mean       0.0    -14.564351          0.001460      0.912598     66.805736   \n",
       "std        0.0     15.274812          0.038183      0.282426    216.581912   \n",
       "min        0.0   -634.000000          0.000000      0.000000  -1622.000000   \n",
       "25%        0.0    -15.000000          0.000000      1.000000      6.000000   \n",
       "50%        0.0    -10.000000          0.000000      1.000000     15.000000   \n",
       "75%        0.0     -8.000000          0.000000      1.000000     44.000000   \n",
       "max        0.0     -6.000000          1.000000      1.000000  14776.000000   \n",
       "\n",
       "       parent_controversiality  \n",
       "count             49999.000000  \n",
       "mean                  0.002740  \n",
       "std                   0.052274  \n",
       "min                   0.000000  \n",
       "25%                   0.000000  \n",
       "50%                   0.000000  \n",
       "75%                   0.000000  \n",
       "max                   1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_neg.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a a fraction of our dataset cleaned and loaded. The next step is to combine the two datasets, and shuffle them. After that we divide the datasets in a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (79998,)\n",
      "X_test: (20000,)\n",
      "y_train: (79998,)\n",
      "y_test: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# First we concatenate both lists and shuffle it to scrabble positive and negative\n",
    "df = pd.concat([df_pos, df_neg])\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Second we split our data for training and testing\n",
    "df['combined'] = df[['text', 'parent_text']].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "text_data = df['combined']\n",
    "text_score = df['score']\n",
    "# parent_text_data = df['parent_text']\n",
    "# parent_text_score = df['parent_score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_data,text_score, test_size = 0.20, random_state = 42)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use a tokenizer which fits all the text in our dataset, it then assigns an integer to each learned word which allows us to convert each entry to a sequence of numbers. These sequences can then be easily passed to our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a tokenizer which will give a word_index integer value to each word\n",
    "tokenizer = Tokenizer(num_words=10000, lower=True, split=' ', document_count=0)\n",
    "\n",
    "# Create the word_index list based on all our data\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "# Now we make a list of sequences of integers based on our texts\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each sentence has a different length and we want to pass the same length vector to our neural network every time, we pad them adding zeros at the end of each sequence so each is 128 integers long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(X_train_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=128)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(X_test_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 128\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]), len(train_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks!\n",
    "\n",
    "Now we can start to build our neural network with TensorFlow. First we create an embedding layer which turns positive integers (indexes) into dense vectors of fixed size. \n",
    "After that we use a GlobalAveragePooling layer which averages all input, this is needed because we feed vectors that contain a lot of zeros, otherwise a lot of neurons will never fire. \n",
    "Then have 2 deep learning layers and in the end one node which will return whether the text that is passed will get a positive or negative score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          2237840   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 2,238,289\n",
      "Trainable params: 2,238,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(8, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(4, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79998 samples, validate on 20000 samples\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/40\n",
      "79998/79998 [==============================] - 4s 46us/sample - loss: 0.6907 - acc: 0.5413 - val_loss: 0.6834 - val_acc: 0.6034\n",
      "Epoch 2/40\n",
      "79998/79998 [==============================] - 3s 40us/sample - loss: 0.6569 - acc: 0.6398 - val_loss: 0.6374 - val_acc: 0.6758\n",
      "Epoch 3/40\n",
      "79998/79998 [==============================] - 3s 35us/sample - loss: 0.6141 - acc: 0.6968 - val_loss: 0.6194 - val_acc: 0.6799\n",
      "Epoch 4/40\n",
      "79998/79998 [==============================] - 3s 32us/sample - loss: 0.5880 - acc: 0.7167 - val_loss: 0.6153 - val_acc: 0.6794\n",
      "Epoch 5/40\n",
      "79998/79998 [==============================] - 3s 32us/sample - loss: 0.5701 - acc: 0.7284 - val_loss: 0.6114 - val_acc: 0.6812\n",
      "Epoch 6/40\n",
      "79998/79998 [==============================] - 3s 32us/sample - loss: 0.5562 - acc: 0.7372 - val_loss: 0.6154 - val_acc: 0.6763\n",
      "Epoch 7/40\n",
      "79998/79998 [==============================] - 3s 32us/sample - loss: 0.5469 - acc: 0.7430 - val_loss: 0.6325 - val_acc: 0.6768\n",
      "Epoch 8/40\n",
      "79998/79998 [==============================] - 3s 40us/sample - loss: 0.5393 - acc: 0.7468 - val_loss: 0.6198 - val_acc: 0.6751\n",
      "Epoch 9/40\n",
      "79998/79998 [==============================] - 3s 35us/sample - loss: 0.5316 - acc: 0.7512 - val_loss: 0.6243 - val_acc: 0.6768\n",
      "Epoch 10/40\n",
      "79998/79998 [==============================] - 3s 36us/sample - loss: 0.5257 - acc: 0.7549 - val_loss: 0.6284 - val_acc: 0.6765\n",
      "Epoch 11/40\n",
      "79998/79998 [==============================] - 3s 34us/sample - loss: 0.5197 - acc: 0.7585 - val_loss: 0.6312 - val_acc: 0.6748\n",
      "Epoch 12/40\n",
      "79998/79998 [==============================] - 3s 39us/sample - loss: 0.5154 - acc: 0.7608 - val_loss: 0.6364 - val_acc: 0.6721\n",
      "Epoch 13/40\n",
      "79998/79998 [==============================] - 3s 39us/sample - loss: 0.5093 - acc: 0.7644 - val_loss: 0.6422 - val_acc: 0.6732\n",
      "Epoch 14/40\n",
      "79998/79998 [==============================] - 3s 33us/sample - loss: 0.5039 - acc: 0.7679 - val_loss: 0.6474 - val_acc: 0.6755\n",
      "Epoch 15/40\n",
      "79998/79998 [==============================] - 3s 33us/sample - loss: 0.4986 - acc: 0.7722 - val_loss: 0.6524 - val_acc: 0.6712\n",
      "Epoch 16/40\n",
      "79998/79998 [==============================] - 3s 32us/sample - loss: 0.4949 - acc: 0.7741 - val_loss: 0.6607 - val_acc: 0.6705\n",
      "Epoch 17/40\n",
      "79998/79998 [==============================] - 3s 33us/sample - loss: 0.4904 - acc: 0.7769 - val_loss: 0.6616 - val_acc: 0.6688\n",
      "Epoch 18/40\n",
      "79998/79998 [==============================] - 3s 32us/sample - loss: 0.4842 - acc: 0.7813 - val_loss: 0.6780 - val_acc: 0.6726\n",
      "Epoch 19/40\n",
      "79998/79998 [==============================] - 3s 34us/sample - loss: 0.4803 - acc: 0.7833 - val_loss: 0.6726 - val_acc: 0.6662\n",
      "Epoch 20/40\n",
      "79998/79998 [==============================] - 3s 32us/sample - loss: 0.4749 - acc: 0.7873 - val_loss: 0.6872 - val_acc: 0.6700\n",
      "Epoch 21/40\n",
      "79998/79998 [==============================] - 3s 38us/sample - loss: 0.4699 - acc: 0.7908 - val_loss: 0.6947 - val_acc: 0.6704\n",
      "Epoch 22/40\n",
      "79998/79998 [==============================] - 3s 34us/sample - loss: 0.4643 - acc: 0.7935 - val_loss: 0.7031 - val_acc: 0.6666\n",
      "Epoch 23/40\n",
      "79998/79998 [==============================] - 3s 36us/sample - loss: 0.4597 - acc: 0.7959 - val_loss: 0.7052 - val_acc: 0.6640\n",
      "Epoch 24/40\n",
      "79998/79998 [==============================] - 3s 38us/sample - loss: 0.4556 - acc: 0.7986 - val_loss: 0.7179 - val_acc: 0.6643\n",
      "Epoch 25/40\n",
      "79998/79998 [==============================] - 3s 37us/sample - loss: 0.4509 - acc: 0.8024 - val_loss: 0.7296 - val_acc: 0.6648\n",
      "Epoch 26/40\n",
      "79998/79998 [==============================] - 3s 34us/sample - loss: 0.4468 - acc: 0.8049 - val_loss: 0.7332 - val_acc: 0.6643\n",
      "Epoch 27/40\n",
      "79998/79998 [==============================] - 3s 35us/sample - loss: 0.4427 - acc: 0.8080 - val_loss: 0.7510 - val_acc: 0.6630\n",
      "Epoch 28/40\n",
      "79998/79998 [==============================] - 3s 34us/sample - loss: 0.4378 - acc: 0.8105 - val_loss: 0.7793 - val_acc: 0.6657\n",
      "Epoch 29/40\n",
      "79998/79998 [==============================] - 3s 34us/sample - loss: 0.4328 - acc: 0.8136 - val_loss: 0.7619 - val_acc: 0.6617\n",
      "Epoch 30/40\n",
      "79998/79998 [==============================] - 3s 35us/sample - loss: 0.4283 - acc: 0.8167 - val_loss: 0.7697 - val_acc: 0.6587\n",
      "Epoch 31/40\n",
      "79998/79998 [==============================] - 3s 35us/sample - loss: 0.4253 - acc: 0.8183 - val_loss: 0.7892 - val_acc: 0.6593\n",
      "Epoch 32/40\n",
      "79998/79998 [==============================] - 3s 34us/sample - loss: 0.4199 - acc: 0.8217 - val_loss: 0.8040 - val_acc: 0.6578\n",
      "Epoch 33/40\n",
      "79998/79998 [==============================] - 3s 34us/sample - loss: 0.4182 - acc: 0.8225 - val_loss: 0.8304 - val_acc: 0.6622\n",
      "Epoch 34/40\n",
      "79998/79998 [==============================] - 3s 35us/sample - loss: 0.4119 - acc: 0.8264 - val_loss: 0.8483 - val_acc: 0.6593\n",
      "Epoch 35/40\n",
      "79998/79998 [==============================] - 3s 34us/sample - loss: 0.4090 - acc: 0.8279 - val_loss: 0.8493 - val_acc: 0.6587\n",
      "Epoch 36/40\n",
      "79998/79998 [==============================] - 3s 35us/sample - loss: 0.4037 - acc: 0.8313 - val_loss: 0.8349 - val_acc: 0.6526\n",
      "Epoch 37/40\n",
      "79998/79998 [==============================] - 3s 35us/sample - loss: 0.3996 - acc: 0.8342 - val_loss: 0.8598 - val_acc: 0.6559\n",
      "Epoch 38/40\n",
      "79998/79998 [==============================] - 3s 35us/sample - loss: 0.3952 - acc: 0.8370 - val_loss: 0.8818 - val_acc: 0.6553\n",
      "Epoch 39/40\n",
      "79998/79998 [==============================] - 3s 35us/sample - loss: 0.3913 - acc: 0.8393 - val_loss: 0.8803 - val_acc: 0.6514\n",
      "Epoch 40/40\n",
      "79998/79998 [==============================] - 3s 34us/sample - loss: 0.3860 - acc: 0.8423 - val_loss: 0.8964 - val_acc: 0.6510\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(test_data, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see our neural network achieves an accuracy of about 85% on our training data and 64% on our test data. Also notice that the accuray of the validation set goes down while that of the training goes up, meaning that we're overfitting our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
