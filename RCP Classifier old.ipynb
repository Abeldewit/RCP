{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "import nltk\n",
    "import glob\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_files = glob.glob(\"data/clean_positive_train_*.csv\")\n",
    "neg_files = glob.glob(\"data/clean_negative_train_*.csv\")\n",
    "\n",
    "df_pos_list = [pd.read_csv(open(fp, 'r'), encoding='utf-8', engine='c') for fp in pos_files[:10]]\n",
    "df_neg_list = [pd.read_csv(open(fp, 'r'), encoding='utf-8', engine='c') for fp in neg_files[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>parent_ups</th>\n",
       "      <th>parent_controversiality</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>profanity</th>\n",
       "      <th>profanity_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>2.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.940550</td>\n",
       "      <td>123.437950</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.036857</td>\n",
       "      <td>0.144450</td>\n",
       "      <td>2.213078e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500013</td>\n",
       "      <td>41.501038</td>\n",
       "      <td>0.031608</td>\n",
       "      <td>0.236471</td>\n",
       "      <td>273.303875</td>\n",
       "      <td>0.045779</td>\n",
       "      <td>0.274485</td>\n",
       "      <td>0.351554</td>\n",
       "      <td>2.556282e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-342.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.092762e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.562152e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.199541e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>129.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.229088e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5295.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score           ups  controversiality  parent_score  \\\n",
       "count  20000.000000  20000.000000      20000.000000  20000.000000   \n",
       "mean       0.500000     32.500000          0.001000      0.940550   \n",
       "std        0.500013     41.501038          0.031608      0.236471   \n",
       "min        0.000000     -9.000000          0.000000      0.000000   \n",
       "25%        0.000000     -9.000000          0.000000      1.000000   \n",
       "50%        0.500000     32.500000          0.000000      1.000000   \n",
       "75%        1.000000     74.000000          0.000000      1.000000   \n",
       "max        1.000000     74.000000          1.000000      1.000000   \n",
       "\n",
       "         parent_ups  parent_controversiality     sentiment     profanity  \\\n",
       "count  20000.000000             20000.000000  20000.000000  20000.000000   \n",
       "mean     123.437950                 0.002100      0.036857      0.144450   \n",
       "std      273.303875                 0.045779      0.274485      0.351554   \n",
       "min     -342.000000                 0.000000     -1.000000      0.000000   \n",
       "25%       10.000000                 0.000000     -0.025000      0.000000   \n",
       "50%       42.000000                 0.000000      0.000000      0.000000   \n",
       "75%      129.250000                 0.000000      0.150000      0.000000   \n",
       "max     5295.000000                 1.000000      1.000000      1.000000   \n",
       "\n",
       "       profanity_prob  \n",
       "count    2.000000e+04  \n",
       "mean     2.213078e-01  \n",
       "std      2.556282e-01  \n",
       "min      1.092762e-20  \n",
       "25%      7.562152e-02  \n",
       "50%      1.199541e-01  \n",
       "75%      2.229088e-01  \n",
       "max      1.000000e+00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = [pd.concat([df_pos, df_neg]) for (df_pos, df_neg) in zip(df_pos_list, df_neg_list)]\n",
    "df = [single_df.sample(frac=1).reset_index(drop=True) for single_df in df]\n",
    "print(len(df))\n",
    "\n",
    "# Now we have scrambeled dataframes of 20000 entries, with features such as binary scores and profanity\n",
    "df[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>parent_ups</th>\n",
       "      <th>parent_controversiality</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>profanity</th>\n",
       "      <th>profanity_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>takes surprising amount energy heat 155 gallon...</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>nothing like climbing 14000 foot mountain gett...</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long doesnt slit throat wife bake pie cool wha...</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>everyone agrees gives good cut right end worth</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.197282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadly upset diggpatriots posts get downvoted s...</td>\n",
       "      <td>0</td>\n",
       "      <td>-9</td>\n",
       "      <td>0</td>\n",
       "      <td>brilliant looking seinfeld reference disappoin...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.062276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>randomly come across cp thread wont get arrest...</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>answered past 4chan asked fbi law enforcement ...</td>\n",
       "      <td>1</td>\n",
       "      <td>297</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.107143</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>switch different brand like neutrogenas</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>eye makeup remover burns like hell gets eyes b...</td>\n",
       "      <td>1</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.118756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  score  ups  \\\n",
       "0  takes surprising amount energy heat 155 gallon...      1   74   \n",
       "1  long doesnt slit throat wife bake pie cool wha...      1   74   \n",
       "2  sadly upset diggpatriots posts get downvoted s...      0   -9   \n",
       "3  randomly come across cp thread wont get arrest...      1   74   \n",
       "4            switch different brand like neutrogenas      1   74   \n",
       "\n",
       "   controversiality                                        parent_text  \\\n",
       "0                 0  nothing like climbing 14000 foot mountain gett...   \n",
       "1                 0     everyone agrees gives good cut right end worth   \n",
       "2                 0  brilliant looking seinfeld reference disappoin...   \n",
       "3                 0  answered past 4chan asked fbi law enforcement ...   \n",
       "4                 0  eye makeup remover burns like hell gets eyes b...   \n",
       "\n",
       "   parent_score  parent_ups  parent_controversiality  sentiment  profanity  \\\n",
       "0             1         178                        0   0.233333          0   \n",
       "1             1          48                        0   0.150000          0   \n",
       "2             1           1                        0  -0.250000          0   \n",
       "3             1         297                        0  -0.107143          0   \n",
       "4             1         197                        0   0.000000          0   \n",
       "\n",
       "   profanity_prob  \n",
       "0        0.041541  \n",
       "1        0.197282  \n",
       "2        0.062276  \n",
       "3        0.059548  \n",
       "4        0.118756  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a small part of clean text we can start to learn the texts to our machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Second we split our data for training and testing\n",
    "text_all = pd.Series()\n",
    "for n in range(len(df)):\n",
    "    text_all = text_all.append(df[n]['text'], ignore_index=True)\n",
    "text_all = text_all.astype(str)\n",
    "print(type(text_all[0]))\n",
    "    \n",
    "text_data_full = [df[n]['text'].astype(str) for n in range(len(df))]\n",
    "text_score_full = [df[n]['score'] for n in range(len(df))]\n",
    "# parent_text_data = df['parent_text']\n",
    "# parent_text_score = df['parent_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19749, 128)\n"
     ]
    }
   ],
   "source": [
    "# We create a tokenizer which will give a word_index integer value to each word\n",
    "tokenizer = Tokenizer(num_words=10000, lower=True, split=' ', document_count=0)\n",
    "\n",
    "# Create the word_index list based on all our data\n",
    "tokenizer.fit_on_texts(text_all)\n",
    "# Now we make a list of sequences of integers based on our texts\n",
    "text_vec = []\n",
    "text_data = []\n",
    "for n in range(len(df)):\n",
    "    text_vec.append(tokenizer.texts_to_sequences(df[n]['text'].astype(str)))\n",
    "    text_data.append(keras.preprocessing.sequence.pad_sequences(text_vec[n], value=0, padding='post', maxlen=128))\n",
    "print(text_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 19749]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-1f04880529a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mx_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mX_train_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 19749]"
     ]
    }
   ],
   "source": [
    "feature_data = [np.array([])]\n",
    "score_data = [np.array([])]\n",
    "\n",
    "X_train = []\n",
    "X_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "for n in range(len(df)):\n",
    "    cur_df = df[n]\n",
    "    print(n)\n",
    "    df[n].dropna(axis=0, inplace=True)\n",
    "    size = len(cur_df)\n",
    "    # Iterate over each row in the DataFrame\n",
    "    f_data = np.array([])\n",
    "    s_data = np.array([])\n",
    "    for index, row in cur_df.iterrows():\n",
    "        \n",
    "        #All features we use\n",
    "        sentiment = row['sentiment']\n",
    "        profanity = row['profanity_prob']\n",
    "        \n",
    "        # Stack the features\n",
    "        features = np.hstack((sentiment, profanity))\n",
    "        \n",
    "        if index == 0:\n",
    "            f_data = features\n",
    "            s_data = row['score']\n",
    "        else:\n",
    "            f_data = np.vstack([f_data, features])\n",
    "            s_data = np.vstack([s_data, row['score']])\n",
    "    if n == 0:\n",
    "        feature_data[0] = f_data\n",
    "        score_data[0] = s_data\n",
    "    else:\n",
    "        feature_data.append(f_data)\n",
    "        score_data.append(s_data)\n",
    "\n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19749, 130)\n"
     ]
    }
   ],
   "source": [
    "x_all = np.hstack((text_data[0],feature_data[0]))\n",
    "print(x_all.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_all, score_data[0], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 128\n",
      "[263   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[ 175 1103   22   47   33  818  348  709  299 2061   29   88   38   90\n",
      "   28   48  212    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "132620\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]), len(train_data[1]))\n",
    "print(max(train_data, key=len))\n",
    "print(max(test_data, key=len))\n",
    "print(len(tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally we can do some neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132620\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          2121920   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                400       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,123,017\n",
      "Trainable params: 2,123,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(24, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/40\n",
      "16000/16000 [==============================] - 1s 57us/sample - loss: 0.6931 - acc: 0.5099 - val_loss: 0.6930 - val_acc: 0.5015\n",
      "Epoch 2/40\n",
      "16000/16000 [==============================] - 1s 38us/sample - loss: 0.6927 - acc: 0.5104 - val_loss: 0.6924 - val_acc: 0.5015\n",
      "Epoch 3/40\n",
      "16000/16000 [==============================] - 1s 38us/sample - loss: 0.6911 - acc: 0.5658 - val_loss: 0.6900 - val_acc: 0.5832\n",
      "Epoch 4/40\n",
      "16000/16000 [==============================] - 1s 41us/sample - loss: 0.6868 - acc: 0.5811 - val_loss: 0.6874 - val_acc: 0.5010\n",
      "Epoch 5/40\n",
      "16000/16000 [==============================] - 1s 38us/sample - loss: 0.6768 - acc: 0.5965 - val_loss: 0.6773 - val_acc: 0.6018\n",
      "Epoch 6/40\n",
      "16000/16000 [==============================] - 1s 38us/sample - loss: 0.6575 - acc: 0.6397 - val_loss: 0.6656 - val_acc: 0.5962\n",
      "Epoch 7/40\n",
      "16000/16000 [==============================] - 1s 38us/sample - loss: 0.6276 - acc: 0.6755 - val_loss: 0.6512 - val_acc: 0.6258\n",
      "Epoch 8/40\n",
      "16000/16000 [==============================] - 1s 39us/sample - loss: 0.5903 - acc: 0.7030 - val_loss: 0.6460 - val_acc: 0.6370\n",
      "Epoch 9/40\n",
      "16000/16000 [==============================] - 1s 38us/sample - loss: 0.5589 - acc: 0.7136 - val_loss: 0.6541 - val_acc: 0.6360\n",
      "Epoch 10/40\n",
      "16000/16000 [==============================] - 1s 38us/sample - loss: 0.5263 - acc: 0.7418 - val_loss: 0.6665 - val_acc: 0.6323\n",
      "Epoch 11/40\n",
      "16000/16000 [==============================] - 1s 40us/sample - loss: 0.5084 - acc: 0.7473 - val_loss: 0.6766 - val_acc: 0.6323\n",
      "Epoch 12/40\n",
      "16000/16000 [==============================] - 1s 38us/sample - loss: 0.4792 - acc: 0.7688 - val_loss: 0.6969 - val_acc: 0.6263\n",
      "Epoch 13/40\n",
      "16000/16000 [==============================] - 1s 36us/sample - loss: 0.4648 - acc: 0.7747 - val_loss: 0.7157 - val_acc: 0.6245\n",
      "Epoch 14/40\n",
      "16000/16000 [==============================] - 1s 36us/sample - loss: 0.4428 - acc: 0.7914 - val_loss: 0.7513 - val_acc: 0.6192\n",
      "Epoch 15/40\n",
      "16000/16000 [==============================] - 1s 37us/sample - loss: 0.4302 - acc: 0.7959 - val_loss: 0.7615 - val_acc: 0.6225\n",
      "Epoch 16/40\n",
      "16000/16000 [==============================] - 1s 37us/sample - loss: 0.4120 - acc: 0.8046 - val_loss: 0.7838 - val_acc: 0.6168\n",
      "Epoch 17/40\n",
      "16000/16000 [==============================] - 1s 36us/sample - loss: 0.4014 - acc: 0.8102 - val_loss: 0.8118 - val_acc: 0.6200\n",
      "Epoch 18/40\n",
      "16000/16000 [==============================] - 1s 36us/sample - loss: 0.3886 - acc: 0.8173 - val_loss: 0.8361 - val_acc: 0.6122\n",
      "Epoch 19/40\n",
      "16000/16000 [==============================] - 1s 36us/sample - loss: 0.3765 - acc: 0.8247 - val_loss: 0.8825 - val_acc: 0.6110\n",
      "Epoch 20/40\n",
      "16000/16000 [==============================] - 1s 35us/sample - loss: 0.3710 - acc: 0.8262 - val_loss: 0.8874 - val_acc: 0.6110\n",
      "Epoch 21/40\n",
      "16000/16000 [==============================] - 1s 34us/sample - loss: 0.3624 - acc: 0.8314 - val_loss: 0.9342 - val_acc: 0.6080\n",
      "Epoch 22/40\n",
      "16000/16000 [==============================] - 1s 34us/sample - loss: 0.3535 - acc: 0.8332 - val_loss: 0.9325 - val_acc: 0.6053\n",
      "Epoch 23/40\n",
      "16000/16000 [==============================] - 1s 34us/sample - loss: 0.3400 - acc: 0.8431 - val_loss: 0.9786 - val_acc: 0.6053\n",
      "Epoch 24/40\n",
      "16000/16000 [==============================] - 1s 35us/sample - loss: 0.3330 - acc: 0.8465 - val_loss: 0.9884 - val_acc: 0.6070\n",
      "Epoch 25/40\n",
      "16000/16000 [==============================] - 1s 35us/sample - loss: 0.3249 - acc: 0.8528 - val_loss: 1.0177 - val_acc: 0.6068\n",
      "Epoch 26/40\n",
      "16000/16000 [==============================] - 1s 35us/sample - loss: 0.3180 - acc: 0.8536 - val_loss: 1.0403 - val_acc: 0.6005\n",
      "Epoch 27/40\n",
      "16000/16000 [==============================] - 1s 34us/sample - loss: 0.3108 - acc: 0.8597 - val_loss: 1.0791 - val_acc: 0.5810\n",
      "Epoch 28/40\n",
      "16000/16000 [==============================] - 1s 36us/sample - loss: 0.3061 - acc: 0.8615 - val_loss: 1.0968 - val_acc: 0.5943\n",
      "Epoch 29/40\n",
      "16000/16000 [==============================] - 1s 35us/sample - loss: 0.3047 - acc: 0.8612 - val_loss: 1.1238 - val_acc: 0.6015\n",
      "Epoch 30/40\n",
      "16000/16000 [==============================] - 1s 35us/sample - loss: 0.2936 - acc: 0.8688 - val_loss: 1.1489 - val_acc: 0.5930\n",
      "Epoch 31/40\n",
      "16000/16000 [==============================] - 1s 33us/sample - loss: 0.2871 - acc: 0.8729 - val_loss: 1.1886 - val_acc: 0.5767\n",
      "Epoch 32/40\n",
      "16000/16000 [==============================] - 1s 37us/sample - loss: 0.2880 - acc: 0.8677 - val_loss: 1.2010 - val_acc: 0.5947\n",
      "Epoch 33/40\n",
      "16000/16000 [==============================] - 1s 38us/sample - loss: 0.2784 - acc: 0.8757 - val_loss: 1.2248 - val_acc: 0.5935\n",
      "Epoch 34/40\n",
      "16000/16000 [==============================] - 1s 33us/sample - loss: 0.2741 - acc: 0.8759 - val_loss: 1.2555 - val_acc: 0.5918\n",
      "Epoch 35/40\n",
      "16000/16000 [==============================] - 1s 36us/sample - loss: 0.2683 - acc: 0.8802 - val_loss: 1.2868 - val_acc: 0.5845\n",
      "Epoch 36/40\n",
      "16000/16000 [==============================] - 1s 35us/sample - loss: 0.2690 - acc: 0.8774 - val_loss: 1.3075 - val_acc: 0.5865\n",
      "Epoch 37/40\n",
      "16000/16000 [==============================] - 1s 34us/sample - loss: 0.2640 - acc: 0.8804 - val_loss: 1.3551 - val_acc: 0.5890\n",
      "Epoch 38/40\n",
      "16000/16000 [==============================] - 1s 33us/sample - loss: 0.2640 - acc: 0.8799 - val_loss: 1.3670 - val_acc: 0.5707\n",
      "Epoch 39/40\n",
      "16000/16000 [==============================] - 1s 38us/sample - loss: 0.2588 - acc: 0.8834 - val_loss: 1.3925 - val_acc: 0.5882\n",
      "Epoch 40/40\n",
      "16000/16000 [==============================] - 1s 35us/sample - loss: 0.2509 - acc: 0.8879 - val_loss: 1.4087 - val_acc: 0.5840\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    y_train,\n",
    "                    epochs=40,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(test_data, y_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
