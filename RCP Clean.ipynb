{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as ts\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originial (2000000, 15)\n",
      "NaN removed (1999977, 15)\n",
      "Originial (2000000, 15)\n",
      "NaN removed (1999951, 15)\n"
     ]
    }
   ],
   "source": [
    "# Reading in the positive voted comments and removing NaN rows\n",
    "df_pos_full = pd.read_csv('comments_positive.csv')\n",
    "print('Originial', df_pos_full.shape)\n",
    "df_pos_full.dropna(axis=0, inplace=True)\n",
    "print('NaN removed',df_pos_full.shape)\n",
    "\n",
    "# Reading in the negative voted comments and removing NaN rows\n",
    "df_neg_full = pd.read_csv('comments_negative.csv')\n",
    "print('Originial', df_neg_full.shape)\n",
    "df_neg_full.dropna(axis=0, inplace=True)\n",
    "print('NaN removed',df_neg_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>ups</th>\n",
       "      <th>author</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>parent_link_id</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>parent_ups</th>\n",
       "      <th>parent_author</th>\n",
       "      <th>parent_controversiality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c092j8m</td>\n",
       "      <td>t1_c092gss</td>\n",
       "      <td>t5_2qh2p</td>\n",
       "      <td>t3_8eyy3</td>\n",
       "      <td>This isn't Twitter: try to comment on the arti...</td>\n",
       "      <td>9582</td>\n",
       "      <td>9582</td>\n",
       "      <td>nraustinii</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_8eyy3</td>\n",
       "      <td>Fucking faggot.</td>\n",
       "      <td>-7526</td>\n",
       "      <td>-7526</td>\n",
       "      <td>Glorificus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c4imcva</td>\n",
       "      <td>t1_c4im948</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>Well, it is exactly what it sounds like. It's ...</td>\n",
       "      <td>9531</td>\n",
       "      <td>9531</td>\n",
       "      <td>Lynfect</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>Elaborate on this cum box, please.</td>\n",
       "      <td>3841</td>\n",
       "      <td>3841</td>\n",
       "      <td>eeeeevil</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c0s4nfi</td>\n",
       "      <td>t1_c0s4lje</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_cf1n2</td>\n",
       "      <td>In soviet Russia, bomb disarms you!</td>\n",
       "      <td>8545</td>\n",
       "      <td>8545</td>\n",
       "      <td>CapnScumbone</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_cf1n2</td>\n",
       "      <td>I don't live in Russia anymore, and I will not...</td>\n",
       "      <td>621</td>\n",
       "      <td>621</td>\n",
       "      <td>shady8x</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c4ini33</td>\n",
       "      <td>t1_c4incln</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>\"runin for senitur! #YOLO!\"</td>\n",
       "      <td>7430</td>\n",
       "      <td>7430</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>This just made me realize that future presiden...</td>\n",
       "      <td>4651</td>\n",
       "      <td>4651</td>\n",
       "      <td>drspg99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c4imgel</td>\n",
       "      <td>t1_c4ima2e</td>\n",
       "      <td>t5_2qh1i</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>You step motherfucker.</td>\n",
       "      <td>7173</td>\n",
       "      <td>7173</td>\n",
       "      <td>jbg89</td>\n",
       "      <td>0</td>\n",
       "      <td>t3_t0ynr</td>\n",
       "      <td>I have sex with my step mom when my dad isn't ...</td>\n",
       "      <td>4251</td>\n",
       "      <td>4251</td>\n",
       "      <td>audir8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   parent_id subreddit_id   link_id  \\\n",
       "0  c092j8m  t1_c092gss     t5_2qh2p  t3_8eyy3   \n",
       "1  c4imcva  t1_c4im948     t5_2qh1i  t3_t0ynr   \n",
       "2  c0s4nfi  t1_c0s4lje     t5_2qh1i  t3_cf1n2   \n",
       "3  c4ini33  t1_c4incln     t5_2qh1i  t3_t0ynr   \n",
       "4  c4imgel  t1_c4ima2e     t5_2qh1i  t3_t0ynr   \n",
       "\n",
       "                                                text  score   ups  \\\n",
       "0  This isn't Twitter: try to comment on the arti...   9582  9582   \n",
       "1  Well, it is exactly what it sounds like. It's ...   9531  9531   \n",
       "2                In soviet Russia, bomb disarms you!   8545  8545   \n",
       "3                        \"runin for senitur! #YOLO!\"   7430  7430   \n",
       "4                             You step motherfucker.   7173  7173   \n",
       "\n",
       "         author  controversiality parent_link_id  \\\n",
       "0    nraustinii                 0       t3_8eyy3   \n",
       "1       Lynfect                 0       t3_t0ynr   \n",
       "2  CapnScumbone                 0       t3_cf1n2   \n",
       "3     [deleted]                 0       t3_t0ynr   \n",
       "4         jbg89                 0       t3_t0ynr   \n",
       "\n",
       "                                         parent_text  parent_score  \\\n",
       "0                                    Fucking faggot.         -7526   \n",
       "1                 Elaborate on this cum box, please.          3841   \n",
       "2  I don't live in Russia anymore, and I will not...           621   \n",
       "3  This just made me realize that future presiden...          4651   \n",
       "4  I have sex with my step mom when my dad isn't ...          4251   \n",
       "\n",
       "   parent_ups parent_author  parent_controversiality  \n",
       "0       -7526    Glorificus                        0  \n",
       "1        3841      eeeeevil                        0  \n",
       "2         621       shady8x                        0  \n",
       "3        4651       drspg99                        0  \n",
       "4        4251        audir8                        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_neg_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dataset consists of two times 2 million comments (rows) so we split it for easier experimenting. \n",
    "# 1% of 2 million = 20000\n",
    "# df_pos = df_pos_full.sample(frac=0.025,random_state=200)\n",
    "# df_neg = df_neg_full.sample(frac=0.025,random_state=200)\n",
    "# df_pos = df_pos.reset_index(drop=True)\n",
    "# df_neg = df_neg.reset_index(drop=True)\n",
    "\n",
    "# print(df_pos.shape)\n",
    "# print(df_neg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're splitting the data in 200 seperate pieces with each 10000 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_split = np.array_split(df_pos_full, 200)\n",
    "df_neg_split = np.array_split(df_neg_full, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in our dataset, the most important columns for our language processing are `text` and `parent_text`\n",
    "\n",
    "So let's get some statistics about these columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_words_pos = df_pos['text'].apply(lambda x: len(x.split()))\n",
    "# num_words_neg = df_neg['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# pos_words_mean, pos_words_std = np.mean(num_words_pos), np.std(num_words_pos)\n",
    "# neg_words_mean, neg_words_std = np.mean(num_words_neg), np.std(num_words_neg)\n",
    "\n",
    "# print(\"Positive stats:\", pos_words_mean, pos_words_std)\n",
    "# print(\"Negative stats:\", neg_words_mean, neg_words_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text, stemming=False, stop_words=True):\n",
    "    import re\n",
    "    from string import punctuation\n",
    "    from nltk.stem import SnowballStemmer\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk import word_tokenize\n",
    "    \n",
    "    stops = stopwords.words('english')\n",
    "    \n",
    "    # Empty comment\n",
    "    if type(text) != str or text=='':\n",
    "        return ''\n",
    "    \n",
    "    # Commence the cleaning!\n",
    "    urls = r'http(s)*:\\/\\/(\\w|\\.)+(\\/\\w+)*'\n",
    "    text = re.sub(urls, '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are\", text)\n",
    "    text = re.sub(\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(\"\\'d\", \" would\", text)\n",
    "    text = re.sub(\"cant\", \"can not\", text)\n",
    "    text = re.sub(\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(\"isn\\'t\", \"is not\", text)\n",
    "    text = re.sub(\"isnt\", \"is not\", text)\n",
    "    text = re.sub(\"whats\", \"what is\", text)\n",
    "    text = re.sub(\"what\\'s\", \"what is\", text)\n",
    "    text = re.sub(\"shouldn't\", \"should not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"I'm\", \"I am\", text)\n",
    "    text = re.sub(\":\", \" \", text)\n",
    "    # The comments contain \\n for line breaks, we need to remove those too\n",
    "    text = re.sub(\"\\\\n\", \" \", text)\n",
    "    \n",
    "    # Special characters\n",
    "    text = re.sub('\\&', \" and \", text)\n",
    "    text = re.sub('\\$', \" dollar \", text)\n",
    "    text = re.sub('\\%', \" percent \", text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = ''.join([word for word in text if word not in punctuation]).lower()\n",
    "    \n",
    "    # If we want to do stemming...\n",
    "    if stemming:\n",
    "        sno = SnowballStemmer('english')\n",
    "        text = ''.join([sno.stem[word] for word in text])\n",
    "    \n",
    "    # If we want to remove stop words...\n",
    "    if stop_words:\n",
    "        text = text.split()\n",
    "        text = [word for word in text if word not in stops]\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making it a binary classification with either positive or negative score\n",
    "def posneg(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from profanity_check import predict as prof_predict\n",
    "from profanity_check import predict_prob\n",
    "from textblob import TextBlob\n",
    "\n",
    "for n in range(200):\n",
    "    \n",
    "    # Clean the text\n",
    "    df_pos_split[n]['text'] = df_pos_split[n]['text'].apply(clean)\n",
    "    df_pos_split[n]['parent_text'] = df_pos_split[n]['parent_text'].apply(clean)\n",
    "    df_neg_split[n]['text'] = df_neg_split[n]['text'].apply(clean)\n",
    "    df_neg_split[n]['parent_text'] = df_neg_split[n]['parent_text'].apply(clean)\n",
    "    \n",
    "    df_pos_split[n]['score'] = df_pos_split[n]['score'].apply(posneg)\n",
    "    df_pos_split[n]['parent_score'] = df_pos_split[n]['parent_score'].apply(posneg)\n",
    "    df_pos_split[n]['sentiment'] = [TextBlob(text).sentiment.polarity for text in df_pos_split[n]['text'].astype(str)]\n",
    "    df_pos_split[n]['profanity'] = [prof_predict([text]) for text in df_pos_split[n]['text'].astype(str)]\n",
    "    df_pos_split[n]['profanity_prob'] = [predict_prob([text]) for text in df_pos_split[n]['text'].astype(str)]\n",
    "    df_pos_split[n]['profanity'] = df_pos_split[n]['profanity'].astype(int)\n",
    "    df_pos_split[n]['profanity_prob'] = df_pos_split[n]['profanity_prob'].astype(float)\n",
    "    \n",
    "    df_neg_split[n]['score'] = df_neg_split[n]['score'].apply(posneg)\n",
    "    df_neg_split[n]['parent_score'] = df_neg_split[n]['parent_score'].apply(posneg)\n",
    "    df_neg_split[n]['sentiment'] = [TextBlob(text).sentiment.polarity for text in df_neg_split[n]['text'].astype(str)]\n",
    "    df_neg_split[n]['profanity'] = [prof_predict([text]) for text in df_neg_split[n]['text'].astype(str)]\n",
    "    df_neg_split[n]['profanity_prob'] = [predict_prob([text]) for text in df_neg_split[n]['text'].astype(str)]\n",
    "    df_neg_split[n]['profanity'] = df_neg_split[n]['profanity'].astype(int)\n",
    "    df_neg_split[n]['profanity_prob'] = df_neg_split[n]['profanity_prob'].astype(float)\n",
    "\n",
    "    df_pos_split[n].dropna(axis=0, inplace=True)\n",
    "    df_neg_split[n].dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # We don't need all columns\n",
    "    col_write = ['text', 'score', 'ups', 'controversiality', 'parent_text', 'parent_score', 'parent_ups', 'parent_controversiality', 'sentiment', 'profanity', 'profanity_prob']\n",
    "    # Save everything in the 'data' folder\n",
    "    p = Path('data/')\n",
    "    number = str(n)\n",
    "    df_pos_split[n].to_csv(Path(p, 'clean_positive_train_' + number + '.csv'), columns=col_write, index=False)\n",
    "    df_neg_split[n].to_csv(Path(p, 'clean_negative_train_' + number + '.csv'), columns=col_write, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 200 clean files for both positive and negative (so 400 total) which we can slowly feed to our neural network to get better and better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
